# 系统分析与设计

## 3.1 系统需求分析与建模

### 3.1.1 系统需求概述

基于BERT的中文电影评论情感分析系统的核心目标，远不止于单纯的情感极性判断。系统旨在构建一个集成化的平台，其需求覆盖了从数据源头的主动获取、复杂网络文本的规范化处理，到核心情感智能分析，最终以直观可视化的方式呈现多维度洞察的全链路流程。具体而言，系统需具备自动化爬取互联网公开电影信息及用户评论的能力，以保证数据的时效性与覆盖面。面对网络文本的非结构化和噪声特性，强大的数据清洗与预处理能力成为刚需，旨在为后续精准分析奠定坚实基础。核心的情感分析功能，要求利用先进的深度学习模型（特指BERT）深度理解中文语境下的细腻情感表达。此外，用户管理和电影管理功能的确立，不仅为评论提供了必要的上下文信息，更是构建个性化用户体验和实现内容有效组织的前提。最终，所有的分析结果和基础数据，都需要通过丰富的可视化图表进行呈现，旨在将复杂的数据转化为易于理解的商业智能或研究洞见，满足不同用户群体（如普通观众、影评人、电影出品方、研究者）的分析需求，提升用户粘性与系统价值。

### 3.1.2 需求清单

表3.1所列的需求功能清单，清晰地勾勒出系统的主要模块及其职责。这些功能并非孤立存在，而是相互关联、层层递进，共同构成了系统的完整服务能力。用户管理（注册、登录、信息管理）是系统运行的基础，为所有用户交互行为提供了身份认证和权限控制，确保了操作的可追溯性和个性化服务的可能性。电影管理（信息展示、搜索、详情）模块则负责构建内容生态的核心，为用户提供浏览、发现和深入了解电影信息的平台，它是评论和情感分析的载体与对象。评论管理（发表、列表、删除）直接关联用户生成内容（UGC）的生命周期，是情感分析的数据来源，也是用户互动的主要途径。情感分析模块作为系统的技术核心，运用BERT模型赋予系统理解文本深层情感的能力，其产出的情感标签和趋势是数据分析模块的重要输入。数据分析模块（用户行为、电影评分、评论情感）则承担着将原始数据和分析结果转化为高价值信息洞察的角色，通过可视化手段揭示用户偏好、电影口碑动态和情感演变规律。最后的系统管理功能（权限、备份）则为系统的稳定运行和安全维护提供了必要保障。这些模块协同工作，形成了一个从数据采集、处理、分析到最终呈现的闭环系统。

### 3.1.3 系统数据分析

在系统数据建模阶段，我们采用Pydantic库来定义核心业务实体的数据结构，这不仅强制实施了严格的数据类型校验，还有效简化了FastAPI框架中API接口的数据验证与序列化过程。以用户数据模型`User`为例，`username`和`email`被设定为必需的字符串类型，其中`email`进一步使用了`EmailStr`类型，确保其符合标准的电子邮件格式规范；`password`字段存储的是经过bcrypt算法加密后的哈希值，而非明文，以保障账户安全；`avatar`字段被定义为`Optional[str]`，允许用户不设置头像；`role`和`status`字段赋予了默认值（'user', 'active'），简化了用户创建流程，同时也为后续的角色权限管理（RBAC）和用户状态控制预留了接口；`created_at`字段使用`datetime`类型记录用户创建的时间戳，便于追踪和分析。同样，电影数据模型`Movie`中，`title`, `director`, `description`等文本信息采用`str`类型，演员`actors`和类型`genres`则设计为`List[str]`，以支持多值存储和查询；`release_date`使用`date`类型，`rating`使用`float`类型，确保了数据的准确性。评论数据模型`Review`则包含了评论内容`content`、用户评分`rating`以及由BERT模型分析得出的情感倾向`sentiment`（通常存储为'positive', 'negative', 'neutral'等字符串），并包含了关联用户`user_id`和电影`movie_id`的外键标识。这些精心设计的Pydantic模型不仅定义了程序内部的数据结构，也直接映射到MongoDB数据库中的文档结构（如`3.2.2`节所述），并作为API请求体和响应体的契约，确保了数据在系统各层流转过程中的一致性与有效性。

### 3.1.4 系统功能分析

系统核心功能流程，如`mermaid`图所示，描绘了典型用户的交互路径。用户首先通过登录或注册接口完成身份认证，这是访问受保护资源（如发表评论、查看个人中心）的前提。认证通过后，用户即可浏览电影列表，通过搜索或筛选功能定位感兴趣的电影。点击电影条目将导航至电影详情页，该页面聚合展示了电影的元数据、简介以及关联的用户评论列表。在详情页，用户可以提交自己的评论和评分，这一操作会触发后端的评论存储和核心的BERT情感分析流程。评论提交成功并经过情感分析后，其结果（情感标签）会即时或异步更新到评论数据中，并可能影响电影的总体情感评分或趋势统计。用户还可以访问个人中心，管理自己发表过的评论（如删除）或查看收藏的电影列表（如果系统包含此功能）。数据可视化模块则面向所有用户或特定权限用户（如管理员），聚合展示从大量评论和用户行为中提炼出的宏观分析结果，例如特定电影的情感分布饼图、情感随时间变化的趋势线图、或热门词云图等。在进行功能分析时，我们也考虑了潜在的扩展点，例如引入基于用户历史行为或协同过滤的电影推荐系统，或增加更复杂的评论互动功能（如点赞、回复、举报），但这些在当前阶段被设定为后续迭代目标。整个流程设计注重用户体验的流畅性和数据的有效利用，确保从用户输入到智能分析再到信息反馈的完整闭环。

## 3.2 系统设计与建模

### 3.2.1 系统设计概述

本系统采纳了当前业界推崇的前后端分离架构模式，旨在实现开发职责的清晰划分、技术栈的灵活选型以及各自独立扩展的可能性。前端选用Vue.js 3框架，特别是其提供的Composition API，极大地提升了代码的可复用性和可维护性，结合Ant Design Vue组件库，快速构建出风格统一、交互丰富的用户界面；路由管理采用Vue Router，状态管理则选择了轻量且高效的Pinia。后端服务基于Python的FastAPI框架构建，其卓越的性能、自动化的API文档生成（基于OpenAPI规范）以及对异步编程的原生支持，使其成为构建高性能API的理想选择；数据持久化方案选定MongoDB，其灵活的文档模型非常适合存储结构可能变化的电影和用户信息，并通过Motor库实现异步数据库访问，与FastAPI的异步特性相得益彰；数据校验则贯穿始终地依赖Pydantic模型。机器学习核心，即情感分析功能，依赖于Hugging Face的Transformers库，方便地加载和微调预训练的`bert-base-chinese`模型。部署层面，系统整体被容器化于Docker环境中，利用Docker Compose编排前端、后端及数据库服务，并通过Nginx作为反向代理服务器，负责处理静态资源请求、负载均衡以及未来可能的HTTPS加密配置。这种分层、解耦的设计确保了系统各部分的高内聚、低耦合，有利于团队协作、独立部署与维护升级。

### 3.2.2 系统数据库设计

数据库设计方面，选择MongoDB作为主要的持久化存储方案，主要是看重其面向文档的特性和良好的水平扩展能力。其灵活的Schema设计允许我们在迭代过程中方便地调整用户、电影或评论的数据结构，而无需执行复杂的数据库迁移操作。具体集合（表）设计如下：`users`集合存储用户信息，其中`username`和`email`字段被设置为唯一索引，以确保用户标识的唯一性，加速登录验证；`password`字段存储的是bcrypt哈希值。`movies`集合存储电影元数据，考虑到用户经常按标题、简介或类型进行搜索，我们为`title`和`description`字段创建了文本索引（Text Index）以支持高效的全文检索，并为`genres`和`actors`这类数组字段建立了多键索引（Multikey Index）以优化基于标签的查询。`reviews`集合存储用户评论，其中`movie_id`和`user_id`字段作为外键关联，建立了普通索引以加速按电影或按用户查询评论的效率；`content`字段同样建立了文本索引；`created_at`字段也建立了索引，用于评论的排序和按时间范围的筛选（如情感趋势分析）。在处理数据关系时，考虑到评论与电影、用户的紧密关联以及查询需求，我们主要采用了引用（Referencing）的方式，即在`reviews`集合中存储`movie_id`和`user_id`，而不是将整个用户或电影文档嵌入评论中，这样可以避免数据冗余和单个文档过大的问题，但在需要关联查询时可能需要额外的数据库查询或应用层聚合。数据库连接采用了异步驱动Motor，以匹配FastAPI的异步特性，提高并发处理能力。

### 3.2.3 系统用户界面设计

系统用户界面（UI）的设计遵循简洁、直观、数据驱动的原则，旨在提供流畅的用户体验（UX）并有效传递信息。我们借助Ant Design Vue提供的丰富、高质量组件，构建了具有一致性和专业性的视觉风格。登录界面 (`UserLogin.vue`) 提供了清晰的输入引导和必要的表单验证（如用户名、密码非空），支持回车提交，并通过按钮的加载状态 (`loading`) 和禁用 (`disabled`) 属性防止重复提交，提升交互友好性。电影列表界面 (`MovieList.vue`) 采用响应式的卡片网格布局 (`a-card`, `a-row`, `a-col`)，在不同屏幕尺寸下都能良好展示；集成了搜索框 (`MovieSearch.vue`) 和基于类型、评分等的筛选器 (`a-select`, `a-dropdown`)，支持即时反馈；分页控件 (`a-pagination`) 实现了数据懒加载，避免一次性加载过多数据。电影详情界面 (`MovieDetail.vue`) 聚合了电影海报 (`a-image`支持预览)、基本信息（使用`a-descriptions`组件结构化展示）、用户评分 (`a-rate`) 以及评论列表 (`ReviewList.vue`)；评论表单 (`ReviewForm.vue`) 提供了易用的输入区域，并可能包含字数限制和提交状态反馈。数据分析界面 (`UserAnalytics.vue`, `MovieAnalytics.vue`, `ReviewAnalytics.vue`) 是UI设计的重点之一，充分利用了ECharts强大的图表绘制能力，生成了如情感分布饼图、评分分布直方图、情感/用户活跃度趋势折线图、词云图等多种交互式图表，并考虑了图表的响应式布局和数据筛选功能，使用户能够直观地探索和理解数据洞察。整体UI设计注重加载性能优化（如图片懒加载、代码分割）和可访问性，力求为所有用户提供良好的使用体验。

### 3.2.4 系统API接口设计

后端API的设计严格遵循RESTful（Representational State Transfer）架构风格，以提供清晰、一致、无状态的接口供前端或其他客户端调用。资源的识别通过URL路径（如`/api/users/me`, `/api/movies/{movie_id}`）实现，操作类型则由HTTP方法（GET, POST, PUT, DELETE）定义。例如，`POST /api/users/register`用于创建新用户，`GET /api/movies`用于获取电影列表，`GET /api/movies/{movie_id}`通过路径参数获取特定电影详情。请求参数的传递方式依据场景选择：路径参数用于标识特定资源，查询参数（如`?skip=0&limit=10`）用于分页、排序或筛选，请求体（Request Body）则用于传输复杂数据结构（如用户注册信息、评论内容），并利用Pydantic模型自动进行数据验证和解析。安全性方面，对于需要认证的接口（如获取用户信息`/api/users/me`、发表评论`/api/reviews`），我们通过FastAPI的依赖注入系统 (`Depends`) 结合OAuth2密码流（Password Flow）和JWT（JSON Web Tokens）实现。前端在登录后获取`access_token`，并在后续请求的`Authorization`头中携带`Bearer <token>`，后端通过`Depends(get_current_user)`这样的依赖项自动验证Token并获取当前用户信息，无效或缺失Token的请求将被拒绝（返回401 Unauthorized）。API响应采用JSON格式，成功操作返回200 OK或201 Created状态码及相应数据，客户端错误（如数据验证失败）返回4xx状态码（如422 Unprocessable Entity，FastAPI自动处理Pydantic验证错误），服务器内部错误返回5xx状态码。接口定义遵循OpenAPI 3.1.0规范（见`docs/openapi.json`），FastAPI能据此自动生成交互式的Swagger UI和ReDoc文档，极大地便利了API的调试、测试和第三方集成。

# 4 系统详细设计与实现

## 4.1 实现环境与工具的简要说明

系统的成功构建与稳定运行，离不开一个明确且一致的开发与部署环境。本节所述的软硬件配置（表4.1）与实现工具（表4.2），不仅记录了开发阶段的具体环境，更重要的是为项目的复现、团队协作以及后续维护提供了基准。选择特定版本的Python (3.8+)、Node.js (14+)、MongoDB (4.4+)、Docker (20.10+) 等核心技术栈，是为了确保库的兼容性、利用特定版本的功能特性，并减少因环境差异导致的问题。例如，选择Python 3.8+是为了利用其在类型提示、异步编程等方面的新特性，这与FastAPI框架的需求高度契合。同样，Node.js 14+对Vue.js 3及其生态系统提供了良好的支持。PyCharm和WebStorm作为专业的IDE，提供了强大的代码编辑、调试、版本控制集成等功能，显著提升了开发效率。Git作为版本控制系统，保障了代码的迭代管理和团队协作的有序进行。定义这些环境与工具，是保证项目工程化、规范化的重要一步。

## 4.2 用户登录模块实现

### 4.2.1 用户认证实现

用户认证作为系统的安全门户，我们采用了基于OAuth2密码流和JWT的认证方案。其核心流程如下：用户在前端提交用户名和密码至`/api/users/token`（或类似登录接口）；后端接收到凭证后，首先从数据库中查询用户是否存在，然后使用`4.2.2`节中描述的`verify_password`函数比对其提交的密码与数据库中存储的哈希值是否匹配。验证通过后，后端将用户的唯一标识（如`user_id`或`username`）以及必要的元数据（如角色`role`、过期时间`exp`）作为Payload，使用预定义的密钥（`SECRET_KEY`）和算法（`ALGORITHM`, 如HS256）生成JWT `access_token`。此Token随后返回给前端。前端通常将Token存储在客户端（如浏览器的localStorage、sessionStorage，或更安全的HttpOnly Cookie中，需配合后端设置）。对于需要认证的API请求，前端必须在HTTP请求的`Authorization`头中以`Bearer <token>`的形式附带此Token。后端FastAPI应用中，我们定义了一个依赖项函数`get_current_user`，它依赖于`OAuth2PasswordBearer(tokenUrl="token")`来自动从请求头中提取Bearer Token。`get_current_user`函数内部负责解码并验证Token的有效性（签名是否正确、是否过期），然后从Token的Payload中提取用户标识（如`sub`字段），并据此查询数据库以获取完整的用户对象。如果Token无效或用户不存在，则抛出HTTPException（通常是401 Unauthorized），中断请求处理；否则，将查询到的用户对象返回。这个用户对象随后可以被注入到路径操作函数中，用于身份识别和权限判断。这种基于Token的无状态认证机制，使得后端服务易于水平扩展。

### 4.2.2 密码加密实现

为保障用户账户安全，系统严禁以明文形式存储用户密码。我们采用了业界广泛推荐的bcrypt算法进行密码哈希处理。具体实现借助了Python的`passlib`库，该库提供了方便易用的接口来处理各种密码哈希算法。我们首先配置一个`CryptContext`实例，指定使用`bcrypt`方案，并可以设置其他参数（如`deprecated="auto"`以支持未来可能的算法升级）。当用户注册或修改密码时，调用`pwd_context.hash(plain_password)`函数。该函数会自动为每个密码生成一个随机的盐（salt），并将盐与密码结合后进行多次哈希运算（bcrypt的计算成本是可配置的，增加了暴力破解的难度），最终生成一个包含算法信息、计算成本、盐和哈希值本身的字符串，存储在数据库的`password`字段中。当用户登录进行密码验证时，调用`pwd_context.verify(plain_password, hashed_password_from_db)`函数。`passlib`会自动从存储的哈希字符串中提取出盐和哈希参数，用相同的盐和参数处理用户输入的明文密码，然后比较生成的哈希值与数据库中存储的哈希值是否一致。这种方式确保了即使数据库泄露，攻击者也无法直接获取用户明文密码，且由于每个密码都有独立的盐，彩虹表攻击也难以奏效。

## 4.3 电影展示模块实现

### 4.3.1 电影数据集爬取

电影数据的获取采用了网络爬虫技术，针对目标网站（如爱奇艺）的公开列表页和详情页进行数据抓取。考虑到爬取任务涉及大量的网络I/O操作，我们选用了Python的`aiohttp`库来实现异步爬虫，以显著提高爬取效率。爬虫程序首先模拟浏览器行为，设置合理的`User-Agent`和其他HTTP头信息，以避免被目标网站识别为爬虫而屏蔽。针对电影列表页，设计了分页处理逻辑，能够自动翻页直至获取所有列表项。获取到列表项中的电影链接后，异步地发起对电影详情页的请求。在解析详情页HTML（可能使用`BeautifulSoup`或`lxml`库）时，需要仔细分析页面结构，使用CSS选择器或XPath定位电影标题、导演、演员、类型、简介、海报图片URL等关键信息。爬取过程中必须健壮地处理各种异常情况，如网络连接错误、请求超时、目标页面结构变更、数据缺失等，实施了重试机制（如指数退避）和详细的日志记录。同时，遵守目标网站的`robots.txt`协议，并设置合理的请求间隔（延时），避免对目标服务器造成过大压力，体现了爬虫的伦理规范。爬取到的原始数据在存入数据库前，会经过`4.3.2`节所述的数据清洗流程。

### 4.3.2 数据清洗

从网络爬取或多源整合的数据往往存在格式不一、信息缺失、包含无关字符等问题，因此数据清洗是保障数据质量、提升后续分析准确性的关键步骤。本系统的数据清洗流程主要针对电影元数据进行，采取了一系列自动化处理规则。例如，对电影标题`title`、导演`director`、简介`description`等文本字段，进行了首尾空白字符去除、特殊HTML实体转义（如`&amp;`转为`&`）、非标准字符过滤等操作。对于演员`actors`和类型`genres`这类通常由特定分隔符（如逗号、斜杠）连接的字符串，进行了标准化拆分，转换为字符串列表，并对列表中的每个元素进行独立清洗（如去除空格）。对于上映日期`release_date`，处理了多种可能的日期格式（如'YYYY-MM-DD', 'YYYY年MM月DD日'），统一转换为标准的ISO日期格式。对于评分`rating`，处理了可能的非数字字符，并转换为浮点数类型。此外，还包括对图片URL`poster`的有效性进行初步检查（如判断是否为合法的URL格式）。数据清洗脚本被设计为幂等的，即对同一份原始数据多次运行清洗脚本，结果应保持一致。对于无法通过自动化规则处理的异常数据或缺失值，会进行记录，并在必要时进行人工干预或设定合理的默认值填充策略。

### 4.3.3 数据展示实现

电影数据的展示主要由前端负责，采用了Vue.js 3和Ant Design Vue组件库。`MovieList.vue`组件承担电影列表的展示任务。该组件在挂载后 (`onMounted`生命周期钩子) 会调用封装好的API请求函数（通常使用`axios`库），异步从后端`/api/movies`接口获取电影数据（包含分页信息）。获取到的数据存储在Pinia管理的全局状态或组件自身的响应式状态 (`ref`, `reactive`) 中。模板部分使用`v-for`指令遍历电影数据列表，将每部电影的信息传递给一个子组件（如`MovieCard.vue`）或直接在循环体内渲染`a-card`。卡片内展示电影海报、标题、导演、评分等关键信息。海报图片使用了图片懒加载技术（如`v-lazy`指令或Intersection Observer API），仅当图片进入可视区域时才加载，优化了页面初始加载性能。分页功能通过监听`a-pagination`组件的页码变化事件，触发新的API请求获取对应页的数据。搜索和筛选功能则通过监听输入框或选择框的变化，更新API请求的查询参数，实现动态过滤或排序。点击电影卡片时，通常使用Vue Router的编程式导航 (`router.push`) 跳转到电影详情页，并将电影ID作为路由参数传递。`MovieDetail.vue`组件则根据路由参数中的电影ID，在挂载时请求`/api/movies/{movie_id}`接口获取详细信息，并将其渲染到页面的各个部分，包括调用`ReviewList.vue`组件来展示关联的评论列表。

## 4.4 电影评论分析模块实现

### 4.4.1 电影评论数据爬取

电影评论数据的获取同样依赖于异步网络爬虫。与电影元数据爬取类似，评论爬取也需模拟浏览器行为、处理分页（评论区通常采用下拉加载或页码分页）、解析HTML结构以提取评论内容、用户名、发布时间、评分（如果有）等信息。评论爬取面临的挑战可能更多，例如许多网站的评论区是动态加载的（可能需要使用Selenium或Playwright等浏览器自动化工具，或者分析其背后的异步请求接口），或者需要用户登录才能查看全部评论。因此，评论爬虫的设计需要更强的鲁棒性和适应性，包括更复杂的页面解析逻辑、对动态内容加载的处理、以及可能的登录状态维持策略。同样，遵守`robots.txt`和设置请求延迟也是必要的。获取到的评论数据会与对应的`movie_id`和`user_id`（如果能获取到）关联，并存储至`reviews`集合，供后续处理和分析。

### 4.4.2 分词处理

中文文本情感分析的首要步骤之一是分词，即将连续的汉字序列切分成具有独立语义的词语单元。本系统选用在中文分词领域广泛使用的`jieba`库来完成此任务。对于每条评论`content`，调用`jieba.cut(text)`方法进行分词。`jieba`支持多种分词模式（如精确模式、全模式、搜索引擎模式），我们通常选用精确模式以获得最准确的切分结果。考虑到电影评论中可能包含领域特定的术语（如电影名、演员名、专业术语）或网络流行语，可以通过`jieba.load_userdict('user_dict.txt')`加载自定义词典，以提高对这些词语的识别准确率。分词后，通常需要进行停用词过滤。停用词是指那些在文本中频繁出现但对语义贡献不大的词语（如"的"、"了"、"是"、标点符号等）。我们维护一个停用词表（`stopwords.txt`），在分词结果中剔除这些词语。这对于后续的词频统计、词云图生成以及一些传统的机器学习模型（如TF-IDF）是重要的预处理步骤。然而，对于基于Transformer的模型（如BERT），由于其能够从上下文中学习词语表示，停用词过滤的重要性相对降低，有时甚至可以省略。处理后的结果通常是以空格分隔的词语字符串，或者是一个词语列表，存储在数据模型的新字段中（如`tokenized_content`），供后续步骤使用。

### 4.4.3 数据格式转换

为了将处理后的文本输入到BERT模型中，必须将其转换为模型能够理解的特定数值格式。这一步主要由Hugging Face Transformers库提供的`BertTokenizer`（在此为`BertTokenizer.from_pretrained('bert-base-chinese')`）完成。对于每条（或一批）经过清洗和分词（或者直接使用原始文本，BERT tokenizer通常能处理）的评论文本，tokenizer执行以下关键操作：首先，将文本切分成模型词汇表（Vocabulary）中存在的Token（对于中文，通常是单个汉字或预定义的词语）。其次，在序列的开头添加特殊的`[CLS]`（Classification）标记，其对应的输出向量通常用于整个序列的分类任务；在序列末尾添加`[SEP]`（Separator）标记。然后，将每个Token映射为其在模型词汇表中的唯一ID（`input_ids`）。接着，生成`attention_mask`，这是一个与`input_ids`等长的二进制序列，用于指示哪些是真实的Token（值为1），哪些是用于填充（Padding）的Token（值为0），模型在计算注意力时会忽略Padding部分。最后，根据设定的最大序列长度（`max_length`，如128或512），对过长的序列进行截断（`truncation=True`），对不足长度的序列进行填充（`padding='max_length'`或`padding='longest'`），确保同一批次内的所有输入序列长度一致。转换后的结果是一个包含`input_ids`, `attention_mask`（以及可能的`token_type_ids`，在此单句分类任务中通常全为0）的字典或对象，可以直接作为PyTorch或TensorFlow模型的输入。

### 4.4.4 情感分析实现

情感分析的核心是利用经过微调（Fine-tuned）的BERT模型进行推理（Inference）。当需要分析一条新的电影评论时，首先执行`4.4.3`节所述的数据格式转换，得到模型所需的输入张量（Tensors）。然后，将这些张量传递给加载好的、处于评估模式 (`model.eval()`) 下的微调模型。模型接收输入后，通过其内部的多层Transformer编码器计算，最终在顶部的分类层输出每个情感类别（积极、中性、消极）的原始得分（Logits）。为了获得概率分布，通常对Logits应用Softmax函数。概率最高的那个类别，即被认为是模型预测的情感倾向。例如，如果Softmax输出为`[0.1, 0.2, 0.7]`，则预测结果为"积极"（假设索引2对应积极）。除了预测的类别标签，我们还可以获取对应的置信度（即最高概率值，此例中为0.7），以及每个类别的具体概率得分，这些信息可以一同存储或返回给前端，用于更细致的展示或分析。整个推理过程应在`torch.no_grad()`上下文管理器中执行，以禁用梯度计算，节省内存并加速计算。对于需要批量处理大量评论的场景，可以将多条评论的输入打包成一个批次（Batch）进行推理，以充分利用硬件并行计算能力，提高整体吞吐量。

### 4.4.5 情感趋势分析

情感趋势分析旨在揭示电影评论情感随时间演变的动态模式。其实现逻辑主要包括数据聚合和时间序列构建。首先，需要确保评论数据（`reviews`集合）中包含准确的时间戳信息（`created_at`字段）。然后，根据分析需求选择时间聚合的粒度，例如按天（Daily）、按周（Weekly）或按月（Monthly）。使用数据库查询（如MongoDB的聚合管道）或在应用层面（如使用Pandas），将评论数据按选定的时间窗口进行分组。在每个时间窗口内，进一步根据评论的情感标签（`sentiment`字段，即由BERT模型预测的结果）进行二次分组，并统计每个情感类别（积极、中性、消极）的评论数量。聚合后的结果通常是一个时间序列数据结构，例如一个列表，其中每个元素代表一个时间点（或区间），并包含该时间点对应的积极、中性、消极评论数。例如：`[{'date': '2024-03-20', 'positive': 150, 'neutral': 80, 'negative': 30}, {'date': '2024-03-21', ...}]`。这个结构化的时间序列数据可以直接传递给前端的图表库（如ECharts），用于绘制多条折线图，每条折线代表一种情感倾向的数量变化，X轴为时间，Y轴为评论数量。通过观察这些曲线的起伏、交叉和相对位置，可以分析口碑发酵过程、特定事件对情感的影响或整体舆论走向。

### 4.4.6 词云图生成

词云图是一种直观展示文本中高频关键词的可视化技术。其生成过程主要涉及文本预处理、词频统计和图像绘制。首先，针对特定电影的所有评论（或者某个时间段内的评论），将其内容（`content`字段）合并成一个大的文本字符串。接着，对这个合并后的文本进行`4.4.2`节所述的分词处理（使用`jieba`）和停用词过滤。分词和过滤后的结果是一个词语列表。然后，使用Python的`collections.Counter`类或类似方法，统计列表中每个词语出现的频率，得到一个词语到其频率的映射（字典）。最后，利用`wordcloud`库来生成词云图像。初始化`WordCloud`对象时，需要配置多个参数，其中最关键的是`font_path`，必须指定一个包含所需中文字符的字体文件路径（如`simhei.ttf`, `msyh.ttf`等），否则中文将显示为乱码方框；其他常用参数包括背景色`background_color`、图像宽度`width`和高度`height`、最大显示词数`max_words`、是否包含词语搭配`collocations`等。配置好`WordCloud`对象后，调用其`generate_from_frequencies(word_frequency_dict)`方法，传入之前计算好的词频字典。该方法会根据词频自动调整词语在图像中的大小和布局。最终，可以通过`to_image()`方法获取PIL图像对象，或使用`to_file(filename)`直接将词云图保存为图片文件（如PNG格式）。生成的词云图可以清晰地展示出评论中被频繁提及的主题或关键词。

## 4.5 用户模块实现

### 4.5.1 管理员页面实现

管理员（Admin）页面是系统管理功能的集中体现，通常只对具有特定角色（如`role='admin'`）的用户开放。其实现涉及到前端的布局、导航、组件以及后端的权限控制。前端方面，通常会有一个专属的管理员布局组件 (`AdminLayout.vue`)，该组件包含一个侧边栏导航菜单 (`a-layout-sider`配合`a-menu`)，列出所有可用的管理功能入口（如用户管理、电影管理、数据分析看板、系统配置等），并使用Ant Design的图标 (`UserOutlined`, `VideoCameraOutlined`等) 增强可识别性。布局的右侧是内容区域 (`a-layout-content`)，其中包含一个`<router-view />`，用于根据侧边栏的导航选择动态加载对应的管理功能组件（如`UserManagement.vue`, `SystemConfig.vue`等）。路由配置层面，所有管理员相关的路由会被组织在一起，并添加路由守卫（Navigation Guard）。该守卫在路由跳转前执行，会检查当前登录用户是否具有管理员权限（通常通过检查本地存储的Token解码后的`role`字段，或者发起一个后端请求验证权限）。若无权限，则重定向到无权限页面或登录页。后端API层面，所有处理管理员操作的接口（如`/api/admin/*`下的接口），都需要在依赖项中加入权限检查逻辑，确保只有管理员角色的用户才能成功调用，例如，在`get_current_user`依赖的基础上增加一个检查用户`role`的依赖。

### 4.5.2 普通用户页面实现

普通用户页面主要指非管理员用户可以访问和操作的界面，如个人信息页 (`UserProfile.vue`)、我发表的评论列表、我的收藏夹等。以`UserProfile.vue`为例，该组件的核心功能是展示和允许用户修改部分个人信息。组件在加载时，会调用后端API（如`GET /api/users/me`）获取当前登录用户的详细信息（用户名、邮箱、头像URL等），并将这些数据显示在表单 (`a-form`) 中。用户名通常是只读的 (`disabled`)。邮箱等可修改字段则与表单输入框 (`a-input`) 进行双向数据绑定 (`v-model`)。头像部分，使用`a-upload`组件允许用户选择本地图片文件。文件选择后，可以进行前端校验（如文件大小、类型），然后通过调用后端专门的头像上传接口（如`POST /api/users/avatar`，通常使用`multipart/form-data`格式）将图片上传至服务器。服务器保存图片后（可能存储在本地文件系统或云存储服务中），更新用户数据库中的`avatar`字段，并返回新的头像URL或其他成功信息。前端在收到成功响应后，更新界面上显示的头像。当用户修改了邮箱等信息并点击保存按钮时，会触发表单提交逻辑，将修改后的数据通过API（如`PUT /api/users/profile`）发送到后端进行更新。整个过程需要包含清晰的用户反馈，如加载状态提示、上传进度显示、保存成功或失败的消息提示 (`a-message`或`a-notification`)。

## 4.6 系统测试

尽管第五章将详细阐述具体的实验评估方法与指标，但在第四章的实现过程中，同步进行的系统测试是确保代码质量和功能正确性的重要环节。我们采取了分层测试策略。在单元测试（Unit Testing）层面，针对后端Python代码中的关键函数（如`4.2.2`节的密码验证函数`verify_password`、`4.3.2`节的数据清洗函数`clean_text`、以及各类工具函数）编写测试用例，使用`pytest`等框架进行测试，确保这些最小功能单元的逻辑正确性。在集成测试（Integration Testing）层面，重点测试后端API接口的行为。利用FastAPI提供的`TestClient`，我们可以模拟HTTP请求，直接调用API端点，并断言其响应状态码、返回数据内容是否符合预期，测试包括带认证和不带认证的场景，验证接口的参数校验、业务逻辑处理以及数据库交互是否正确。对于前端，虽然本章未详述，但也可以进行组件级别的单元测试（如使用Vue Test Utils）和集成测试（测试组件间的交互）。性能测试（Performance Testing），如`4.6.2`节代码片段所示，使用`aiohttp`或其他工具（如Locust）模拟大量并发用户请求特定的API接口（如获取电影列表`/api/movies`），测量系统的响应时间、吞吐量（QPS）和错误率，以评估系统在高负载下的表现，并据此进行性能瓶颈分析和优化。功能测试（Functional Testing）则更侧重于端到端（End-to-End）的用户场景验证，确保整个系统作为一个整体能够按预期工作，这部分虽然主要在`4.6.3`和`4.6.4`中提及，但其测试用例的设计和执行贯穿于开发实现阶段。这种多层次的测试方法有助于及早发现并修复缺陷，提升软件质量。

## 4.7 本章小结

...

# 5 实验方法

## 5.1 数据采集与预处理

本研究实验所使用的数据集源自对国内主流在线视频平台"爱奇艺"的公开电影信息及用户评论的定向爬取。通过精心设计的异步网络爬虫程序，共采集到覆盖1018部不同类型电影的元数据，以及与之关联的高达3,272,529条用户评论文本，形成了大规模、领域特定的原始语料库。考虑到网络爬取数据的固有噪声与异构性，数据预处理成为保障后续模型训练有效性的关键步骤。预处理流程首先聚焦于数据清洗，采用自动化脚本与人工校验相结合的方式，严格剔除重复的电影条目与评论记录，处理用户昵称、评论内容中的缺失值，并对电影类型（`genre`）、演员列表等字段进行标准化格式转换，例如统一分隔符、去除首尾空白字符等，以确保数据的一致性与规整性。依据`dataset/data_structure.md`中定义的JSON格式，对清洗后的数据进行结构化组织。更为关键的是，由于原始评论数据缺乏显式的情感标签，我们采用了基于规则与弱监督学习相结合的方法进行了初步的情感倾向标注（积极、中性、消极），并抽取部分样本进行人工精标与交叉验证，以评估自动标注的质量并迭代优化标注策略，最终形成用于模型训练和评估的标注数据集。此过程充分考虑了中文网络用语的复杂性，如谐音、反讽、表情符号等，力求提升标注的准确性与鲁棒性。

## 5.2 BERT模型训练

在情感分析模型的选型上，本研究决定采用在自然语言处理领域表现卓越的BERT（Bidirectional Encoder Representations from Transformers）模型。具体而言，我们选用了`bert-base-chinese`预训练模型，该模型基于海量中文语料进行训练，对中文的语法结构和语义特征具有深刻的理解能力，且其参数规模（约1.1亿）在性能与计算资源消耗之间取得了较好的平衡，适合在现有硬件条件下进行微调。模型训练的核心在于微调（Fine-tuning）过程，即将预训练好的BERT模型应用于下游的特定任务——电影评论情感分类。我们解冻预训练模型的参数，并在其顶部添加一个简单的分类层（通常是全连接层加Softmax激活函数），然后使用经过预处理和标注的电影评论数据集对整个模型（或仅部分层）进行进一步训练。这个过程使得模型能够学习到电影评论领域特有的语言模式和情感表达方式。超参数的选择对模型性能至关重要，如`5.2.2`节代码片段所示，我们通过实验确定了合适的训练轮数（`num_train_epochs=3`），以在充分学习和防止过拟合间取得平衡；设定了设备训练批次大小（`per_device_train_batch_size=16`）和评估批次大小（`per_device_eval_batch_size=64`），以兼顾GPU显存限制和梯度估计的稳定性；采用了带有预热（`warmup_steps=500`）的学习率调度策略和权重衰减（`weight_decay=0.01`），以帮助模型更快收敛并提高泛化能力。损失函数的构建（`5.2.3`节）采用了适用于多分类任务的标准交叉熵损失函数（`CrossEntropyLoss`），它通过计算模型预测概率分布与真实标签分布之间的差异来指导模型参数的优化方向，有效惩罚错误预测。模型优化（`5.2.4`节）方面，除了上述学习率调整和权重衰减策略，我们还引入了早停（Early Stopping）机制，通过监控模型在独立验证集上的性能指标（如准确率或F1分数），在指标不再提升时提前终止训练，从而有效避免模型在训练集上过拟合。同时，对批量大小的选择也进行了细致考量，平衡了训练速度、内存占用和模型的泛化性能。未来工作中，还可以考虑模型集成（Ensemble Methods）等策略，通过结合多个独立训练的模型预测结果，进一步提升整体情感分析的稳定性和准确性。

## 5.3 BERT情感分析方法

本研究将电影评论的情感倾向定义为三分类问题，具体划分为积极（Positive）、中性（Neutral）和消极（Negative）三类。这种分类体系（`5.3.1`节）既能捕捉评论中明确的褒贬态度，也能识别那些客观陈述、观点模糊或包含混合情感的评论，相较于二分类模型具有更强的表达力和现实适用性。积极评论通常包含喜爱、推荐、赞扬等正面词汇；消极评论则表达不满、批评、失望等负面情绪；中性评论则可能涉及客观剧情描述、无明显情感色彩的评价或褒贬并存的情况。情感判别的核心架构（`5.3.2`节）基于微调后的BERT模型。输入一条电影评论文本后，首先通过`bert-base-chinese`的分词器（Tokenizer）将其转换为模型可接受的输入格式（包括Token ID、Segment ID和Attention Mask）。随后，这些输入被送入BERT模型进行编码，模型通过其多层Transformer结构捕捉文本中的深层语义信息和上下文依赖关系。特别地，BERT模型输出的`[CLS]`标记对应的隐藏状态向量被认为蕴含了整个序列的概要信息，非常适合用于文本分类任务。该向量被输入到模型顶部的线性分类层，经过Softmax函数归一化后，输出对应积极、中性、消极三个类别的概率分布。概率最高的类别即被判定为该评论的情感倾向。然而，单纯依赖模型的最高概率输出可能不足以全面理解评论的情感内涵。因此，我们提出了一种综合分析策略（`5.3.3`节），该策略不仅考虑模型输出的情感得分和置信度（可以通过Softmax输出概率来衡量），设置合理的判别阈值，还会结合评论的具体上下文信息进行判断。例如，识别和处理文本中的反讽、隐喻等复杂语言现象，甚至可以引入用户历史评论行为或电影本身的元数据作为辅助信息。此外，对特定电影或整体评论的情感进行时间趋势分析，监测情感随时间（如电影上映前后、口碑发酵过程）的变化模式，识别异常波动点，能够提供更宏观和动态的情感洞察。

## 5.4 BERT模型评估

为了科学、全面地评估所训练的BERT情感分析模型的性能，本研究采用了一系列标准的机器学习评估方法和指标。首先，混淆矩阵（Confusion Matrix）（`5.4.1`节）是评估分类模型性能的基础工具。通过构建混淆矩阵，我们可以清晰地观察模型在各个类别上的预测表现，具体包括每个类别的真正例（True Positives）、假正例（False Positives）、真反例（True Negatives）和假反例（False Negatives）的数量。这不仅有助于计算后续的各项指标，还能直观地揭示模型容易混淆的类别（例如，是否经常将中性评论误判为积极或消极），为模型改进提供方向。为了获得更可靠和稳健的模型性能估计，避免单次划分训练集和测试集带来的偶然性，我们采用了K折交叉验证（K-Fold Cross-Validation）策略（`5.4.2`节），实验中设定K=5。该方法将整个标注数据集随机划分为5个互斥的子集，每次使用其中4个子集作为训练数据，剩下的1个子集作为验证数据，重复5次，确保每个子集都被用作验证集一次。最终的模型性能指标是这5次验证结果的平均值，同时计算标准差以衡量性能的稳定性。这种方法能更充分地利用数据，降低评估结果的方差，提供对模型泛化能力更可信的度量。在具体的评估指标（`5.4.3`节）选择上，我们关注了多个维度：准确率（Accuracy）衡量了模型整体预测正确的比例，是基本评估指标，但在类别分布不均衡时可能产生误导；精确率（Precision）关注被模型预测为某个特定类别的样本中，实际真正属于该类别的比例，反映了模型预测的"查准"能力；召回率（Recall）则衡量了某个特定类别的所有样本中，被模型成功预测出来的比例，反映了模型的"查全"能力；F1分数（F1-Score）是精确率和召回率的调和平均数，旨在平衡这两者，尤其适用于类别不平衡或对精确率和召回率同等重视的场景。此外，我们还绘制了受试者工作特征曲线（ROC Curve）并计算曲线下面积（AUC），ROC曲线通过展示在不同分类阈值下模型的真阳性率（Recall）与假阳性率（1-Specificity）的关系，能够全面评估模型在所有可能阈值下的性能表现，AUC值则提供了单一数值来衡量模型的整体区分能力，AUC越接近1，表示模型性能越好。选择和分析这些多样的指标，有助于我们从不同角度深入理解模型的优势与不足。